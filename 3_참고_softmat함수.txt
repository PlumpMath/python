# softmax(소프트맥스)는 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수이다.
# 분류하고 싶은 클래수의 수 만큼 출력으로 구성한다.가장 큰 출력 값을 부여받은 클래스가 확률이 가장 높은 것으로 이용된다.
# 그러나,소프트맥스 결과값이 [0.4, 0.3, 0.2, 0.1]으로 나와 1등한 0.4와
#  [0.7, 0.1, 0.1, 0.1]으로 나와 1등한 0.7은 다를 것이므로 그 정도에 따라 추가 판단하기도 한다.
# 자세히 살펴보면 입력값의 대소 순서가 출력값의 대소 순서와 같다는 것을 알 수 있다.
# 결국 가장 큰 값은 이미 소프트맥스 이전에 가장 큰 값이였다.
# 따라서 추론(운영)단계에서 연산속도를 빠르기하기 위해 생략하기도 한다.
# 소프트맥스 결과값을 One hot encoder의 입력으로 연결하면
# 가장 큰 값만 True값, 나머지는 False값이 나오게 하여 이용 가능하다.



import numpy as np
import matplotlib.pyplot as plt
 
.
# 소프트맥스 지수승사용 단, np.max를 하는 이유는 지수함수의 특성상 값이 급격히 커져 overflow를 조금이라도 방지하기 위해
# 입력값중 최대값을 이용하여 모든값에서 빼고자 함 동일하게 뺀경우 계산값의 차이없으므로 max값 제외하고 사용가능
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()


# 소프트맥스는 1과 2가 아닌 e^1 = 2.718,  e^2 = 7.389 값을 계산에 이용한다.
# 즉 입력값이 커짐에 따라 기울기가 증가하며 더 큰 차이가 발생한다.
# 따라서 0.2, 0.2, 0.6이 나온 것이다
 
x = np.array([1.0,1.0,2.0])   #1행(1.)  2행(1.0) 3행(2.0)
# x=[1,2,3]
y = softmax(x)
print("x===>",x)
print("y===", y)
print(np.sum(y))
 
ratio = y
labels = y
 
plt.pie(ratio, labels=labels, shadow=True, startangle=90)
plt.show()