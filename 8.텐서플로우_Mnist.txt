# https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/mnist/tf/



from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data.read_data_sets("c:/py_data/tensor_mnist/", one_hot=True)


## 훈련 전용 데이터 이미지
mnist.train.images
len(mnist.train.images)   #55,000건 나옴.


# 훈련전용데이터 레이블목록 / 텐서플로우는 배열형태로 저장됨.
# , 단위로 10개의 0과 1이 출력됨   (위치값에 따라 숫자 0~9가 표현되는 것임)
# 예) 숫자0은   1,0,0,0,0,0,0,0,0,0
# 예) 숫자 3은  0,0,1,0,0,0,0,0,0,0



mnist.train.labels  
mnist.train.labels[7]   #0~54,9999 배열리스트중 8번째자료의 값출력  
mnist.train.labels[2]   #0~54,9999 배열리스트중 3번째자료의 값출력

#############----------------------------------------------------

import tensorflow as tf
pixels=28*28    #5*5 필터사용할 예정임 입력1개 출력32
nums=10  # 0-9사이의 카테고리


### 플레이스 홀더 생성
x=tf.placeholder(tf.float32, shape=(None, pixels), name="X") #이미지데이터
y_=tf.placeholder(tf.float32, shape=(None, nums),name="y_") #정답레이블

### 가중치와 바이어스를 초기화함
def weight_variable(name,shape):
    W_init=tf.truncated_normal(shape, stddev=0.1)
    W=tf.Variable(W_init, name="W_"+name)
    return W

def bias_variable(name, size):
    b_init=tf.constant(0.1, shape=[size])
    b=tf.Variable(b_init, name="b_"+name)
    return b


#### 합성곱층1   Relu는 입력이 0 이하라면 0, 0보다 클때만 해당값 출력
with tf.name_scope("conv1") as scope:  #name_scope(레이어이름)
    W_conv1=weight_variable("conv1",[5,5,1,32])
    b_conv1=bias_variable("conv1",32)
    x_image=tf.reshape(x,[-1,28,28,1])
    h_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)
    
    
### 합성곱 계층 작성
def conv2d(x,W):
    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding="SAME")


# 최대풀링층을 만듦
def max_pool(x):
    return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1],padding="SAME")

# 풀링층1
with tf.name_scope("pool1") as scope:
    h_pool1=max_pool(h_conv1)
    
#### 합성곱층2
with tf.name_scope("conv2") as scope:  #name_scope(레이어이름)
    W_conv2=weight_variable("conv2",[5,5,32,64])
    b_conv2=bias_variable("conv2",64)
    x_image=tf.reshape(x,[-1,28,28,1])
    h_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)

# 풀링층2    풀링층을 두번 통과하므로 이미지의 크기는 28/2/2가 되어 7*7이됨.
with tf.name_scope("pool2") as scope:
    h_pool2=max_pool(h_conv2)



#### ------------------------------------------------------------
## 과잉적합 막기
with tf.name_scope("dropout") as scope:
    keep_prob=tf.placeholder(tf.float32)
    h_fc_drop=tf.nn.dropout(h_fc,keep_prob)


##출력층
with tf.name_scope("readout") as scope:
    W_fc2=weight_variable("fc2",[1024,10])
    b_fc2=bias_variable("fc2",10)
    y_conv=tf.nn.softmax(tf.matmul(h_fc_drop, W_fc2)+b_fc2)


######-----------------------------------------------
# 모델학습
with tf.name_scope("loss") as scope:
    cross_entoropy=-tf.reduce_sum(y_*tf.log(y_conv))
with tf.name_scope("training") as scope:
    optimizer=tf.train.AdamOptimizer(1e-4)  #확률적 경사하강법: 무작위로 초기화한 매개변수를 손실함수가 작아지도록 지속적으로 반복해서 변경함.
    train_step=optimizer.minimize(cross_entoropy)
    
# 모델평가
with tf.name_scope("predict") as scope:
    predict_step=tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))
    accuracy_step=tf.reduce_mean(tf.cast(predict_step,tf.float32))
    
# 전결합층
with tf.name_scope("fully_connected") as scope:
    n=7*7*64
    W_fc=weight_variable("fc",[n,1024])
    b_fc=bias_variable("fc",1024)
    h_pool2_flat=tf.reshape(h_pool2,[-1,n])   #풀링층2의 자료로 작업
    h_fc=tf.nn.relu(tf.matmul(h_pool2_flat,W_fc)+b_fc)  #
    


###-----------------------------------------------------------------------------
##
feed_diclt 설정  플레이스 홀더 
def set_feed(images, labels, prob):
    return{x:images, y_:labels, keep_prob: prob}
print("a")
#세션시작
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    test_fd=set_feed(mnist.test.images,mnist.test.labels,1)  #테스트전용 피드작성
    
    ###학습시작
    for step in range(100):
        batch=mnist.train.next_batch(50)
        fd=set_feed(batch[0],batch[1],0.5)
        loss=sess.run([train_step,cross_entoropy],feed_dict=fd)
        print(step)
        if step % 10 ==0:
            acc=sess.run(accuracy_step, feed_dict=test_fd)
            print("step=",step,"loss=",loss,"acc=",acc)
### 최종결과확인
acc=sess.run(accuracy_step, feed_dict=test_fd)
print("정답률=",acc)

print("-----end------------")
            




 