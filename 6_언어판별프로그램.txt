#위키피디아에서 무작위로 기사를 추출하고 각 언어의 식별자를 나누어 저장함.
# 영어(en), 프랑스어(fr), 인도네시아어(id), 타갈로그어(tl)을 사용해 테스트해봄

from sklearn import svm, metrics
import glob, os.path, re, json
# 텍스트를 읽어 들이고 출현 빈도 조사하기 --- 텍스트파일을 읽음, 알파벳의 출현빈도를 조사함.
# 파일이름앞의 2글자가 언어를 나타내는 라벨이므로, 언어 ID를 파일에서 2글자 추출함(en-1, fr-6에서 en,fr)


def check_freq(fname):
    name = os.path.basename(fname)
    lang = re.match(r'^[a-z]{2,}', name).group()
    with open(fname, "r", encoding="utf-8") as f:
        text = f.read()
    text = text.lower() # 소문자 변환 (또는 대문자) 대소문자 구별하지 않기 위하여 

    # 숫자 세기 변수(cnt) 초기화하기
    cnt = [0 for n in range(0, 26)]
    code_a = ord("a")
    code_z = ord("z")


    # 알파벳 출현 횟수 구하기(알바텟 외의 글자 무시함)
    #  --- 한글자씩 읽어서 문자코드로 변환, 해당코드를 기반으로 배열의 특정위치에 있는 숫자를 한개씩 증가함.즉 a~z까지 나온 글자갯수 누적
    
    for ch in text:
        n = ord(ch)
        if code_a <= n <= code_z: # a~z 사이에 있을 때
            cnt[n - code_a] += 1


    # 정규화하기 --- 
    # 알파벳의출현 횟수를 글자수의 합계로 나누어 출현 빈도로 변환

    total = sum(cnt)
    freq = list(map(lambda n: n / total, cnt))
    return (freq, lang)
    
### 각 파일 처리하기
def load_files(path):
    freqs = []
    labels = []
    file_list = glob.glob(path)
    for fname in file_list:
        r = check_freq(fname)
        freqs.append(r[0])
        labels.append(r[1])
    return {"freqs":freqs, "labels":labels}
data = load_files("c:/py_data/lang/train/*.txt")
test = load_files("c:/py_data/lang/test/*.txt")
# 이후를 대비해서 JSON으로 결과 저장하기
with open("c:/py_data/lang/freq.json", "w", encoding="utf-8") as fp:
    json.dump([data, test], fp)



# 학습하기 --- SVM 으로 학습
clf = svm.SVC()
clf.fit(data["freqs"], data["labels"])


# 예측하기 --- 테스트전용데이터를 이용하여
predict = clf.predict(test["freqs"])


# 결과 테스트하기 --- 결과보기
ac_score = metrics.accuracy_score(test["labels"], predict)
cl_report = metrics.classification_report(test["labels"], predict)
print("정답률 =", ac_score)
print("리포트 =")
print(cl_report)